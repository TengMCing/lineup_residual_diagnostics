% interactcadsample.tex
% v1.03 - April 2017

\documentclass[]{interact}

\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage{subfigure}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required

\usepackage{natbib}% Citation support using natbib.sty
\bibpunct[, ]{(}{)}{;}{a}{}{,}% Citation support using natbib.sty
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}% Bibliography support using natbib.sty

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}


% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



\usepackage{lscape}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\def\tightlist{}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\begin{document}


\articletype{ARTICLE TEMPLATE}

\title{A Plot is Worth a Thousand Tests: Assessing Residual Diagnostics
with the Lineup Protocol}


\author{\name{Weihao Li$^{a}$, Dianne Cook$^{a}$, Emi Tanaka$^{a, b,
c}$, Susan VanderPlas$^{d}$}
\affil{$^{a}$Department of Econometrics and Business Statistics, Monash
University, Clayton, VIC, Australia; $^{b}$Biological Data Science
Institute, Australian National University, Acton, ACT,
Australia; $^{c}$Research School of Finance, Actuarial Studies and
Statistics, Australian National University, Acton, ACT,
Australia; $^{d}$Department of Statistics, University of Nebraska,
Lincoln, Nebraska, USA}
}

\thanks{CONTACT Weihao
Li. Email: \href{mailto:weihao.li@monash.edu}{\nolinkurl{weihao.li@monash.edu}}, Dianne
Cook. Email: \href{mailto:dicook@monash.edu}{\nolinkurl{dicook@monash.edu}}, Emi
Tanaka. Email: \href{mailto:emi.tanaka@anu.edu.au}{\nolinkurl{emi.tanaka@anu.edu.au}}, Susan
VanderPlas. Email: \href{mailto:susan.vanderplas@unl.edu}{\nolinkurl{susan.vanderplas@unl.edu}}}

\maketitle

\begin{abstract}
Abstract to fill.
\end{abstract}

\begin{keywords}
statistical graphics; data visualization; visual inference; hypothesis
testing; reression analysis; cognitive perception; simulation; practical
significance; effect size
\end{keywords}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\begin{quote}
\emph{``Since all models are wrong the scientist must be alert to what
is importantly wrong.''} \citep{box1976science}
\end{quote}

Diagnostics are the key to determining whether there is anything
importantly wrong with a model. In linear regression analysis, residuals
from the model fit are commonly used. Residuals summarise what is not
captured by the model, and thus provide the capacity to identify what
might be wrong.

We can assess residuals in multiple ways. Residuals may be plotted as a
histogram or quantile-quantile plot to examine the distribution. Using
the classical normal linear regression model as an example, if the
distribution is symmetric and unimodal, we consider it to be well
behaved. However, if the distribution is skewed, bimodal, multimodal, or
contains outliers, there is a cause for concern. One could also inspect
the distribution by conducting a goodness of fit test, such as the
Shapiro-Wilk Normality test \citep{shapiro1965analysis}.

More typically, residuals will be plotted, as a scatter plot against the
predicted values and each of the explanatory variables to scrutinize
their relationships. If there are any visually discoverable patterns,
the model is potentially misspecified. In general, one looks for
noticeable departures from the model like non-linear dependency or
heteroskedasticity. However, correctly judging a residual plot where no
pattern exists can be a painstakingly difficult task for humans. It is
especially common, particularly among students, to misinterpret patterns
that are random noise and random deviation from a model
\citep{loy2021bringing}. It is also possible to conduct hypothesis tests
for non-linear dependence \citep{ramsey_tests_1969}, and use a
Breusch-Pagan test \citep{breusch_simple_1979} for heteroskedasticity.

Abundance of literature describe appropriate diagnostic methods for
linear regression, e.g. \citet{draper1998applied},
\citet{montgomery1982introduction}, \citet{belsley_regression_1980},
\citet{cook_applied_1999} and \citet{cook1982residuals}. All these
writings consider plotting residuals to be a standard technique that
should be examined routinely in all regression modelling problems. In
addition, \citet{draper1998applied} and \citet{belsley_regression_1980}
believe that residual plots are usually revealing when the assumptions
are violated. \citet{cook_applied_1999} thinks formal tests and
graphical procedures are complementary and both have a place in residual
analysis, but they focus on graphical methods rather than on formal
testing, as they are easier to use. \citet{montgomery1982introduction}
even suggests that residual plots are more informative in most practical
situations than the corresponding formal tests, and statistical tests on
regression model residuals are not widely used based on their
experience.

A common guidance by experts is that optimal method for diagnosing model
fits is by plotting the data. The persistence of this advice to check
the plots is curious, and investigating why this might be common advice
is the subject of this paper. The paper is structured as follows. The
next section describes the background on the types of departures that
one expects to detect, and outlines a formal statistical process for
reading residual plots, called visual inference. Section
\ref{experimental-design} details the experimental design to compare the
decisions made by formal hypothesis testing, and how humans would read
diagnostic plots. The results are reported in Section \ref{results}. We
conclude with a discussion of future work, in particular, how the
responsibility for residual plot reading might be relieved.

\hypertarget{background}{%
\section{Background}\label{background}}

\hypertarget{departures-from-good-residual-plots}{%
\subsection{Departures from good residual
plots}\label{departures-from-good-residual-plots}}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/residual-plot-common-departures-1} 

}

\caption{Example residual vs fitted value plots: (A) classically good looking residuals, (B) non-linear pattern indicates that the model has not captured a non-linear association, (C) heteroskedasticity indicating that variance around the fitted model is not uniform, and (D) non-normality where the residual distribution is not symmetric around 0. The latter pattern might best be assessed using a univariate plot of the residuals, but patterns B and C need to be assessed using a residual vs fitted value plot.}\label{fig:residual-plot-common-departures}
\end{figure}

Graphical summaries in which residuals are plotted against fitted values
or other functions of the predictors that are approximately orthogonal
to the residuals are referred to as standard residual plots in
\citet{cook1982residuals}. Figure
\ref{fig:residual-plot-common-departures}A shows an ideal residual plot
where the residuals are evenly distributed at both sides of the
horizontal zero line, with no noticeable patterns.

There are various types of departures from an ideal residual plot.
Non-linearity, heteroskedasticity and non-normality are perhaps the
three mostly checked departures.

Non-linearity is a type of model misspecification caused by failing to
include higher order terms of the predictors in the regression equation.
Any non-linear functional form of residuals on fitted values in the
residual plot could be indicative of non-linearity. An example residual
plot containing visual pattern of non-linearity is shown in Figure
\ref{fig:residual-plot-common-departures}B. One can clearly observe the
``S-shape'' from the residual plot as the cubic term is not captured by
the misspecified model.

Heteroskedasticity refers to the presence of nonconstant error variance
in a regression model. It is mostly due to the strict but false
assumptions on the variance-covariance matrix of the error term. The
usual pattern of heteroskedasticity on a residual plot is the
inconsistent spread of the residuals across the horizontal axis.
Visually, it sometimes results in the so-called ``butterfly'' shape as
shown in Figure \ref{fig:residual-plot-common-departures}C, or the
``left-triangle'' and ``right-triangle'' shape where the smallest
variance occurs at one side of the horizontal axis.

Compared to non-linearity and heteroskedasticity, non-normality is
usually harder to detect from a residual plot since a scatter plot do
not readily reveal the marginal distribution. A favourable graphical
summary for this task is the quantile-quantile plot. As we mainly
discuss residual plots, non-normality will not be the focus of this
paper. For a consistent comparison, the residual plot of this departure
is still presented in Figure \ref{fig:residual-plot-common-departures}D.
When the number residuals below and above the horizontal axis are uneven
across the local regions along the \(x\)-axis, we expect that the
normality assumption is violated. For example, given a skewed error
distribution, there will be fewer data points and more outliers on one
side of the horizontal axis as shown in Figure
\ref{fig:residual-plot-common-departures}D.

\hypertarget{conventionally-testing-for-departures}{%
\subsection{Conventionally testing for
departures}\label{conventionally-testing-for-departures}}

Many different hypothesis tests are available to detect specific model
defects. For example, the presence of heteroskedasticity can usually be
tested by applying the White test
\citep{white_heteroskedasticity-consistent_1980} or the Breusch-Pagan
test \citep{breusch_simple_1979}, which are both derived from the
Lagrange multiplier test \citep{silvey1959lagrangian} principle that
relies on the asymptotic properties of the null distribution. To test
specific forms of non-linearity, one may apply the F-test as a model
structural test to examine the significance of specific polynomial and
non-linear forms of the predictors, or the significance of proxy
variables as in the Ramsey Regression Equation Specification Error Test
(RESET) \citep{ramsey_tests_1969}. The Shapiro-Wilk test
\citep{shapiro1965analysis} is the most widely used test of
non-normality included by many of the statistical software programs. The
Jarque--Bera test \citep{jarque1980efficient} is also used to directly
check whether the sample skewness and kurtosis match a normal
distribution.

Table \ref{tab:example-residual-plot-table} displays the \(p\)-values
from the RESET, Breusch-Pagan and Shapiro-Wilk tests applied to the
residual plots in Figure \ref{fig:residual-plot-common-departures}. The
RESET test and Breusch-Pagan test were computed using the
\texttt{resettest} and \texttt{bptest} functions from the R package
\texttt{lmtest}, respectively. The Shapiro-Wilk test was computed using
the \texttt{shapiro.test} from the core R package \texttt{stats}. (The
the R package \texttt{skedastic} \citep{skedastic} contains a large
collection of tests for heteroskedasticity.) Although, the RESET test is
exact, it requires the selection of a power parameter. According to
\citet{ramsey_tests_1969}, a power of four is recommended, which we
adopted in our analysis. The Breusch-Pagan and Shapiro-Wilk tests are
approximate.

We would expect the RESET test for non-linearity to reject residual plot
B, the Breusch-Pagan test for heteroskedasticity to reject the residual
plot C, and Shapiro-Wilk test for non-normality to reject residual plot
D, which they all do and correctly fail to reject residual plot A.
Interestingly, the Breusch-Pagan and Shapiro-Wilk tests also reject the
residual plots exhibiting structure that they weren't designed for.
\citet{cook1982residuals} discusses that most residual-based tests for
particular types of departure from model assumptions are also sensitive
to other types of departures, that is, correctly rejected but for the
wrong reason, a phenomenon known as the ``Type III error''. Also, some
types of departure can also have elements of other types of departure,
for example, non-linearity could be viewed as heteroskedasticity.
Additionally, other problesms such as outliers can trigger the rejection
\citep{cook_applied_1999}.

With large sample sizes, hypothesis tests are may reject a residual plot
when there is only a slight departure (XXX is there a reference for this
statement?). While such rejections may be statistically correct, their
sensitivity may render the results impractical. Therefore, a key goal of
residual plot diagnostics is to identify potential issues that could
lead to incorrect conclusions or errors in subsequent analyses. However,
minor defects in the model are unlikely to have a significant impact and
may be best disregarded for practical purposes. In fact, the experiment
discussed in this paper specifically addresses this.

\begin{table}

\caption{\label{tab:example-residual-plot-table}Statistical significance testing for departures from good residuals for plots in Figure \ref{fig:residual-plot-common-departures}. Shown are the $p$-values calculated for the RESET, the Breusch-Pagan and the Shapiro–Wilk tests. The good residual plot (A) is judged a good residual plot, as expected, by all tests. The non-linearity (B) is detected by all tests, as might be expected given the extreme structure.}
\centering
\begin{tabular}[t]{llrrr}
\toprule
Plot & Departures & RESET & Breusch-Pagan & Shapiro–Wilk\\
\midrule
A & None & 0.779 & 0.133 & 0.728\\
B & Non-linearity & \em{0.000} & \em{0.000} & \em{0.039}\\
C & Heteroskedasticity & 0.658 & \em{0.000} & \em{0.000}\\
D & Non-normality & 0.863 & 0.736 & \em{0.000}\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{visual-test-procedure-based-on-lineups}{%
\subsection{Visual test procedure based on
lineups}\label{visual-test-procedure-based-on-lineups}}

The examination of data plots to infer signal or patterns (or lack
thereof) is fraught with variation in the human ability to interpret and
decode the information embedded in graph
\citep{cleveland_graphical_1984}. Such examination thus feels there is a
subjective nature to evaluate, say, diagnostic plots to infer
appropriateness of statistical models, although arguably we would expect
such evaluation should be done objectively and such human-to-human
variation in ``reading'' data plots should be minimised.

In practice, over-interpretation of a single plot is common. For
instance, in \citet{roy_chowdhury_using_2015}, some over-interpreted the
separation between gene groups in a two-dimensional projection of a
linear discriminant analysis when in fact there are no differences in
the expression levels between the gene groups. To mitigate this
over-interpretation, \citet{buja_statistical_2009} proposed a line-up
protocol to assess plots in a manner analogous to the null hypothesis
significance testing (NHST) framework. More specifically, the protocol
consists of \(m\) randomly placed plots, where one plot is the data
plot, and the remaining \(m - 1\) plots, referred to as the \emph{null
plots}, have the identical graphical procedure as the data plot except
the data is replaced with a data generation mechanism that is consistent
with the null hypothesis, \(H_0\). Then, an observer who have not seen
the data plot will be asked to point out the most different plot from
the lineup. Under \(H_0\), it is expected that the data plot would have
no distinguishable difference from the null plots, and the probability
that the observer correctly picks the data plot is \(1/m\). If one
rejects \(H_0\) as the observer correctly picks the data plot, then the
Type I error of this test is \(1/m\). This protocol requires apriori
specification of \(H_0\) (or at least a null data generating mechanism)
much like the requirement of knowing the distribution of the test
statistic in NHST.

\begin{figure}[t!]

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/first-example-lineup-1} 

}

\caption{Visual testing is conducted using a lineup, as in the example here. The residual plot computed from the observed data (plot $2^2 + 2$, exhibiting non-linearity) is embedded among 19 null plots, where the residuals are simulated from a standard error model. Computing the $p$-value requires that the lineup be examined by a number of human judges, each asked to select the most different plot. A small $p$-value would result from a substantial number selecting plot $2^2 + 2$.}\label{fig:first-example-lineup}
\end{figure}

Figure \ref{fig:first-example-lineup} is an example of a lineup
protocol. If the data plot at position \(2^2 + 2\) is identifiable, then
it is evidence for the rejection of \(H_0\). In fact, the actual
residual plot is obtained from a misspecified regression model with
missing non-linear terms.

Data used in the \(m - 1\) null plots needs to be simulated. In
regression diagnostics, sampling data consistent with \(H_0\) is
equivalent to sampling data from the assumed model. As
\citet{buja_statistical_2009} suggested, \(H_0\) is usually a composite
hypothesis controlled by nuisance parameters. Since regression models
can have various forms, there is no general solution to this problem,
but it sometimes can be reduced to a so called ``reference
distribution'' by applying one of the three methods: (i) sampling from a
conditional distribution given a minimal sufficient statistic under
\(H_0\), (ii) parametric bootstrap sampling with nuisance parameters
estimated under \(H_0\), and (iii) Bayesian posterior predictive
sampling. The conditional distribution given a minimal sufficient
statistic is the best justified reference distribution among the three
\citep{buja_statistical_2009}. Essentially, null residuals can be
simulated by regressing \(N\) i.i.d standard normal random draws on the
predictors, then rescaling it by the ratio of residual sum of square in
two regressions.

The effectiveness of lineup protocol for regression analysis is
validated by \citet{majumder_validation_2013} under relatively simple
settings with up to two predictors. Their results suggest that visual
tests are capable of testing the significance of a single predictor with
a similar power as a t-test, though they express that in general it is
unnecessary to use visual inference if there exists a conventional test,
and they do not expect the visual test to perform equally well as the
conventional test. In their third experiment, where there is not a
conventional test, visual test outperforms the conventional test for a
large margin. This is encouraging, as it promotes the use of visual
inference in situations where there are no existing statistical testing
procedures. Visual inference have also been integrated into diagnostic
of hierarchical linear models by \citet{loy2013diagnostic},
\citet{loy2014hlmdiag} and \citet{loy2015you}. They use lineup protocols
to judge the assumption of linearity, normality and constant error
variance for both the level-1 and level-2 residuals.

\hypertarget{calculation-of-statistical-significance-and-test-power}{%
\section{Calculation of statistical significance and test
power}\label{calculation-of-statistical-significance-and-test-power}}

\hypertarget{what-is-being-tested}{%
\subsection{What is being tested?}\label{what-is-being-tested}}

In diagnosing a model fit from residuals, we are generally interested in
\emph{the regression model is correctly specified} (\(H_0\)) against the
broad alternative \emph{the regression model is misspecified} (\(H_a\)).

However, it is practically impossible to test this specific \(H_0\) with
conventional tests, which are constructed to measure specific
departures. For example, the RESET test is formulated as
\(H_0:\gamma_1 = \gamma_2 = \gamma_3 = 0\) against
\(H_a: \gamma_1 \neq 0 \text{ or } \gamma_2 \neq 0 \text{ or } \gamma_3 \neq 0\),
from
\(y = \tau_0 + \sum_{i=1}^{p}\tau_px_p +\gamma_1\hat{y}^2 + \gamma_2\hat{y}^3 + \gamma_3\hat{y}^4 + u, ~~u \sim N(0, \sigma_u^2)\).
Similarly, the Breusch-Pagan test is designed to specifically test
\(H_0:\) \emph{error variances are all equal}
(\(\zeta_i=0 \text{ for } i=1,..,p\)) versus the alternative \(H_a:\)
\emph{that the error variances are a multiplicative function of one or
more variables} (\(\text{at least one } \zeta_i\neq 0\)) from
\(e^2 = \zeta_0 + \sum_{i=1}^{p}\zeta_i x_i + u, ~ u\sim N(0,\sigma_u^2)\).

One of the potential benefits of the visual test, based on the lineup
protocol, is that it works as an omnibus test, able to detect a range of
departures from good residuals.

\hypertarget{statistical-significance}{%
\subsection{\texorpdfstring{Statistical
significance\textasciitilde{}\label{sig}}{Statistical significance\textasciitilde{}}}\label{statistical-significance}}

In hypothesis testing, a \(p\)-value is defined as the probability of
observing test results as least as extreme as the observed result given
\(H_0\) is true. Conventional hypothesis tests usually have an existing
method to derive or compute \(p\)-value based on the null distribution.
What we need to discuss in the following is the method to estimate
\(p\)-value for a visual test.

Within the context of visual inference, by involving \(k\) independent
observers, the visual \(p\)-value can be interpreted as the probability
of having as many or more subjects detect the data plot than the
observed result.

Let \(X_j = \{0,1\}\) be a Bernoulli random variable denoting whether
subject \(j\) correctly detecting the data plot, and
\(X = \sum_{j=1}^{K}X_j\) be the number of observers correctly picking
the data plot. Then, by imposing a relatively strong assumption on the
visual test that all \(K\) evaluations are fully independent, under
\(H_0\), \(X \sim \mathrm{Binom}_{K,1/m}\). Therefore, the \(p\)-value
of a lineup of size \(m\) evaluated by \(K\) observer is given as
\(P(X \geq x) = 1 - F(x) + f(x)\), where \(F(.)\) is the binomial
cumulative distribution function, \(f(.)\) is the binomial probability
mass function and \(x\) is the realization of number of observers
correctly picking the data plot \citep{majumder_validation_2013}.

As pointed out by \citet{vanderplas2021statistical}, this basic binomial
model does not take into account the possible dependencies in the visual
test due to repeated evaluations of the same lineup. And it is
inapplicable to visual test where subjects are asked to select one or
more ``most different'' plots from the lineup.
\citet{vanderplas2021statistical} summarises three common scenarios in
visual inference: (1) \(K\) different lineups are shown to \(K\)
subjects, (2) \(K\) lineups with different null plots but the same data
plot are shown to \(K\) subjects, and (3) the same lineup is shown to
\(K\) subjects. Out of these three scenarios, Scenario 3 is the most
common in previous studies as it puts the least constraints on the
experiment design. For Scenario 3, \citet{vanderplas2021statistical}
models the probability of a plot \(i\) being selected from a lineup as
\(\theta_i\), where \(\theta_i \sim Dirichlet(\alpha)\) for
\(i=1,...,m\) and \(\alpha > 0\). The number of times plot \(i\) being
selected in \(K\) evaluations is denoted as \(c_i\). In case subject
\(j\) makes multiple selections, \(1/s_j\) will be added to \(c_i\)
instead of one, where \(s_j\) is the number of plots subject \(j\)
selected for \(j=1,...K\). This ensures \(\sum_{i}c_i=K\). Since we are
only interested in the selections of the data plot \(i\), the marginal
model can be simplified to a beta-binomial model and thus the visual
\(p\)-value is given as

\begin{equation} \label{eq:pvalue-beta-binomial}
P(C \geq c_i) = \sum_{x=c_i}^{K}{K \choose x}\frac{B(x + \alpha, K - x + (m - 1)\alpha)}{B(\alpha, (m-1)\alpha)},\quad \text{for} \quad c_i \in \mathbb{Z}_0^+
\end{equation}

\noindent where \(B(.)\) is the beta function defined as

\begin{equation} \label{eq:betafunction}
B(a, b) = \int_{0}^{1}t^{\alpha - 1}(1-t)^{b-1}dt,\quad \text{where}\quad a,b>0.
\end{equation}

Note that Equation \ref{eq:pvalue-beta-binomial} given in
\citet{vanderplas2021statistical} only works with non-negative integer
\(c_i\). We extend the equation to non-negative real number \(c_i\) by
applying a linear approximation

\begin{equation} \label{eq:pvalue-beta-binomial-approx}
P(C \geq c_i) = P(C \geq \lceil c_i \rceil) + (\lceil c_i \rceil - c_i) P(C = \lfloor c_i \rfloor), \quad \text{for}\quad c_i \in \mathbb{R}_0^+,
\end{equation}

\noindent where \(P(C \geq \lceil c_i \rceil)\) is calculated using
Equation \ref{eq:pvalue-beta-binomial} and
\(P(C = \lfloor c_i \rfloor)\) is calculated by

\begin{equation} \label{eq:pmf-beta-binomial}
P(C = c_i) = {K \choose c_i}\frac{B(c_i + \alpha, K - c_i + (m - 1)\alpha)}{B(\alpha, (m-1)\alpha)},\quad \text{for} \quad c_i \in \mathbb{Z}_0^+.
\end{equation}

Besides, the parameter \(\alpha\) used in Equation
\ref{eq:pvalue-beta-binomial} and \ref{eq:pmf-beta-binomial} is usually
unknown and hence needs to be estimated from the survey data. For low
values of \(\alpha\), only a few plots are attractive to the observers
and tend to be selected. For higher values of \(\alpha\), the
distribution of the probability of each plot being selected is more
even. \citet{vanderplas2021statistical} defines that a plot is
\(c\)-interesting if \(c\) or more participants select the plot as the
most different. Given the definition, The expected number of plots
selected at least \(c\) times, \(E[Z_c]\), is calculated as

\begin{equation} \label{eq:c-interesting-expectation}
E[Z_c(\alpha)] = \frac{m}{B(\alpha, (m-1)\alpha)}\sum_{\lceil c \rceil}^{K}{K \choose x} B(x + \alpha, K - x + (m-1)\alpha).\end{equation}

With Equation \ref{eq:c-interesting-expectation}, \(\alpha\) can be
estimated using maximum likelihood estimation. But for precise estimate
of \(\alpha\), additional human responses to Rorschach lineups, which is
a type of lineup that consists of plots constructed from the same null
data generating mechanism, are required.

\hypertarget{power-of-the-tests}{%
\subsection{Power of the tests}\label{power-of-the-tests}}

The power of a model misspecification test is the probability that
\(H_0\) is rejected given the regression model is misspecified in a
specific way. It is an important indicator when one is concerned about
whether model assumptions have been violated. Although in practice, one
might be more interested in knowing how much the residuals deviate from
the model assumptions, and whether this deviation is of practical
significance.

The power of a conventional hypothesis test is affected by both the true
parameter \(\boldsymbol{\theta}\) and the sample size \(n\). These two
can be quantified in terms of effect size \(E\) to measure the strength
of the residual departures from the model assumptions. Details about the
effect size is provided in Section \ref{effect-size} after the
introduction of the simulation model used in our human subject
experiment. The theoretical power of a test is sometimes not a trival
solution, but it can be estimated if the data generating process is
known. We use a predefined model to generate a large set of simulated
data under different effect sizes, and record if the conventional test
rejects \(H_0\). The probability of the conventional test rejects
\(H_0\) is then fitted by a logistic regression formulated as

\begin{equation} \label{eq:logistic-regression-1-1}
Pr(\text{reject}~H_0|H_1,E) = \Lambda\left(log\left(\frac{0.05}{0.95}\right) + \beta_1 E\right),
\end{equation}

\noindent where \(\Lambda(.)\) is the standard logistic function given
as \(\Lambda(z) = exp(z)/(1+exp(z))\). The effect size \(E\) is the only
predictor and the intercept is fixed to \(log(0.05/0.95)\) so that
\(\hat{Pr}(\text{reject}~H_0|H_1,E = 0) = 0.05\), which is the desired
significance level.

The power of a visual test on the other hand, may additionally depend on
the ability of the particular subject, as the skill of the individual
may affect the number of observers who identify the data plot from the
lineup \citep{majumder_validation_2013}. To address this issue,
\citet{majumder_validation_2013} models the probability of a subject
\(j\) correctly picking the data plot from a lineup \(l\) using a
mixed-effect logistic regression, with subjects treated as random
effect. Then, the estimated power of a visual test evaluated by a single
subject is the predicted value obtained from the mix-effect model.
However, this mix-effect model does not work with scenario where
subjects are asked to select one or more most different plots. In this
scenario, having the probability of a subject \(j\) correctly picking
the data plot from a lineup \(l\) is insufficient to determine the power
of a visual test because it does not provide information about the
number of selections made by the subject for the calculation of the
\(p\)-value (See Equation \ref{eq:pvalue-beta-binomial-approx}).
Therefore, we directly estimate the probability of a lineup being
rejected by assuming that individual skill has negligible effect on the
variation of the power. This assumption is not necessary true, but it
helps simplifying the model structure, thereby obviate a costly
large-scale experiment to estimate complex covariance matrices. The same
model given in Equation \ref{eq:logistic-regression-1-1} is applied to
model the power of a visual test.

To study various factors contributing to the power of both tests, the
same logistic regression model is fit on different subsets of the
collated data grouped by levels of factors. These include the
distribution of the fitted values, type of the simulation model and the
shape of the residual departures.

\hypertarget{experimental-design}{%
\section{Experimental design}\label{experimental-design}}

An experiment is conducted in three data collection periods to
investigate the difference between conventional hypothesis testing and
visual inference in the application of linear regression diagnostics.
Two types of departures, non-linearity and heteroskedasticity, are
collected during data collection periods I and II. The data collection
period III was designed primarily to measure human responses to null
lineups so that the parameter \(\alpha\) in Equation
\ref{eq:pvalue-beta-binomial} can be estimated. Additional lineups for
both non-linearity and heteroskedasticity, using uniform fitted value
distribution, were included so that the participants were evaluating
some lineups with signal also. It would be too frustrating for
participants to only be assigned lineups with all null plots. Overall,
we collected 7974 evaluations on 1152 unique lineups performed by 443
subjects throughout three data collection periods.

\hypertarget{simulating-departures-from-good-residuals}{%
\subsection{Simulating departures from good
residuals}\label{simulating-departures-from-good-residuals}}

\hypertarget{non-linearity}{%
\subsubsection{Non-linearity}\label{non-linearity}}

Data collection period I is designed to study the ability of human
subjects to detect the effect of a non-linear term \(\boldsymbol{z}\)
constructed using Hermite polynomials on random vector
\(\boldsymbol{x}\) formulated as

\begin{align} \label{eq:nonlinearity-model}
\boldsymbol{y} &= 1 + \boldsymbol{x} + \boldsymbol{z} + \boldsymbol{\varepsilon},\\
\boldsymbol{x} &= g(\boldsymbol{x}_{raw}, 1), \\
\boldsymbol{z} &= g(\boldsymbol{z}_{raw}, 1), \\
\boldsymbol{z}_{raw} &= He_j(g(\boldsymbol{x}, 2)),
\end{align}

\noindent where \(\boldsymbol{y}\), \(\boldsymbol{x}\),
\(\boldsymbol{\varepsilon}\), \(\boldsymbol{x}_{raw}\),
\(\boldsymbol{z}_{raw}\) are vectors of size \(n\), \(He_{j}(.)\) is the
\(j\)th-order probabilist's Hermite polynomials,
\(\varepsilon \sim N(\boldsymbol{0}, \sigma^2\boldsymbol{I}_n)\), and
\(g(\boldsymbol{x}, k)\) is a scaling function to enforce the support of
the random vector to be \([-k, k]^n\) defined as

\begin{equation} \label{eq:scaling-function}
g(\boldsymbol{x}, k) = (\boldsymbol{x} - min(\boldsymbol{x}))/max(\boldsymbol{x} - min(\boldsymbol{x}))2k - k, \quad \text{for} \quad k > 0. 
\end{equation}

According to \citet{abramowitz1964handbook}, Hermite polynomials were
initially defined by \citet{de1820theorie}, but named after Hermite
\citep{hermite1864nouveau} because of the unrecognisable form of
Laplace's work. When simulating \(\boldsymbol{z}_{raw}\), function
\texttt{hermite} from the R package \texttt{mpoly} \citep{mpoly} is used
to generate Hermite polynomials.

The null regression model used to fit the realizations generated by the
above model is formulated as

\begin{equation} \label{eq:null-model}
\boldsymbol{y} = \beta_0 + \beta_1 \boldsymbol{x} + \boldsymbol{u},
\end{equation}

\noindent where
\(\boldsymbol{u} \sim N(\boldsymbol{0}, \sigma^2\boldsymbol{I}_n)\).

Since \(z = O(x^j)\), for \(j > 1\), \(z\) is a higher order term leaves
out by the null regression, which will lead to model misspecification.

Visual patterns of non-linearity are simulated using four different
orders of probabilist's Hermite polynomials (\(j = 2, 3, 6, 18\)). (A
summary of the factors is given in Table \ref{tab:model-factor-table}.)
The values of \(j\) is chosen so that distinct shapes of non-linearity
are included in the residual plot. These include ``U'', ``S'', ``M'' and
``triple-U'' shape as shown in Figure
\ref{fig:different-shape-of-herimite}. A greater value of \(j\) will
result in a curve with more turning points. It is expected that the
``U'' shape will be the easiest one to detect because complex shape
tends to be concealed by cluster of data points.

Figure \ref{fig:example-poly-lineup} demonstrates one of the lineups
used in non-linearity detection. This lineup is produced by the
non-linearity model with \(j = 6\). The data plot location is
\(2^3 - 4\). All five subjects correctly identify the data plot from
this lineup.

\begin{table}

\caption{\label{tab:model-factor-table}Levels of the factors used in data collection periods I, II, III.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{rr|rr|rcrr|rr|rcrr|rr|rcrr|rr|rcrr|rr|rcrr|rr|rc}
\toprule
\multicolumn{2}{c}{Non-linearity} & \multicolumn{2}{c}{Heteroskedasticity} & \multicolumn{2}{c}{Common} \\
\cmidrule(l{3pt}r{3pt}){1-2} \cmidrule(l{3pt}r{3pt}){3-4} \cmidrule(l{3pt}r{3pt}){5-6}
\multicolumn{1}{c}{Poly Order ($j$)} & \multicolumn{1}{c}{SD ($\sigma$)} & \multicolumn{1}{c}{Shape ($a$)} & \multicolumn{1}{c}{Ratio ($b$)} & \multicolumn{1}{c}{Size ($n$)} & \multicolumn{1}{c}{Distribution of fitted values} \\
\cmidrule(l{3pt}r{3pt}){1-1} \cmidrule(l{3pt}r{3pt}){2-2} \cmidrule(l{3pt}r{3pt}){3-3} \cmidrule(l{3pt}r{3pt}){4-4} \cmidrule(l{3pt}r{3pt}){5-5} \cmidrule(l{3pt}r{3pt}){6-6}
2 & 0.25 & -1 & 0.25 & 50 & Uniform\\
3 & 1.00 & 0 & 1.00 & 100 & Normal\\
6 & 2.00 & 1 & 4.00 & 300 & Skewed\\
18 & 4.00 &  & 16.00 &  & Discrete\\
 &  &  & 64.00 &  & \\
\bottomrule
\end{tabular}}
\end{table}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/different-shape-of-herimite-1} 

}

\caption{Polynomial forms generated for the residual plots used to assess detecting non-linearity. The four shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$.}\label{fig:different-shape-of-herimite}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/different-sigma-1} 

}

\caption{Examining the effect of $\sigma$ on the signal strength in the non-linearity detection, for $n=300$, uniform fitted value distribution and the "U" shape. As $\sigma$ increases the signal strength decreases, to the point that the "U" is almost unrecognisable when $\sigma=4$.}\label{fig:different-sigma}
\end{figure}

\begin{figure}[t!]

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/example-poly-lineup-1} 

}

\caption{One of the lineups containing non-linearity patterns used in data collection period I. Can you spot the most different plot? The data plot is positioned at $2^3 + 1$.}\label{fig:example-poly-lineup}
\end{figure}

\hypertarget{heteroskedasticity}{%
\subsubsection{Heteroskedasticity}\label{heteroskedasticity}}

Data collection period II is designed to study the ability of human
subjects to detect the appearance of a heteroskedasticity pattern under
a simple linear regression model setting:

\begin{align} \label{eq:heter-model}
\boldsymbol{y} &= 1 + \boldsymbol{x} + \boldsymbol{\varepsilon},\\
\boldsymbol{x} &= g(\boldsymbol{x}_{raw}, 1),\\
\boldsymbol{\varepsilon} &\sim N(\boldsymbol{0}, 1 + (2 - |a|)(\boldsymbol{x} - a)^2b \boldsymbol{I}), 
\end{align}

\noindent where \(\boldsymbol{y}\), \(\boldsymbol{x}\),
\(\boldsymbol{\varepsilon}\) are vectors of size \(n\) and \(g(.)\) is
the scaling function defined in Equation \ref{eq:scaling-function}.

The null regression model used to fit the realizations generated by the
above model is formulated exactly the same as Equation
\ref{eq:null-model}.

For \(b \neq 0\), the variance-covariance matrix of the error term
\(\boldsymbol{\varepsilon}\) is correlated with the predictor
\(\boldsymbol{x}\), which will lead to the presence of
heteroskedasticity. Visual patterns of heteroskedasticity are simulated
using three different shapes (\(a\) = -1, 0, 1). (A summary of the
factors can be found in Table \ref{tab:model-factor-table}.)

Since \(supp(X) = [-1, 1]\), choosing \(a\) to be \(-1\), \(0\) and
\(1\) can generate ``left-triangle'', ``butterfly'' and
``right-triangle'' shape as displayed in Figure
\ref{fig:different-shape-of-heter}. The term \((2 - |a|)\) maintains the
magnitude of residuals across different values of \(a\).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/different-shape-of-heter-1} 

}

\caption{Heteroskedasticity forms used in the experiment. Three different shapes ($a = -1, 0, 1$) are used in the experiment to create left-triangle, "butterfly" and "right-triangle" shapes, respectively.}\label{fig:different-shape-of-heter}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/different-b-1} 

}

\caption{Five different values of $b$ are used in heteroskedasticity simulation to control the strength of the signal. Larger values of $b$ yield a bigger difference in variation, and stus stronger heteroskedasticity signal.}\label{fig:different-b}
\end{figure}

\begin{figure}[t!]

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/example-heter-lineup-1} 

}

\caption{One of the lineups containing heteroskedasticity pattern used in data collection period II. Can you spot the most different plot? The data plot is positioned at $3^3 - 3^2$}\label{fig:example-heter-lineup}
\end{figure}

An example lineup of this model used in data collection period II is
shown in Figure \ref{fig:example-heter-lineup} with \(a = -1\). The data
plot location is \(2^4 + 2\). Nine out of 11 subjects correctly identify
the data plot from this lineup.

\hypertarget{factors-common-to-both-data-collection-periods}{%
\subsubsection{Factors common to both data collection
periods}\label{factors-common-to-both-data-collection-periods}}

Fitted values are a function of the independent variables, and the
distribution of the observed values affects the distribution of the
fitted values. In the best case scenario the fitted values will have a
uniform distribution, which means that there is even coverage of
possible observed values across all of the predictors. This is not
always present in the collected data. Sometimes the fitted values are
discrete because one or more predictors were measured discretely. The
distribution may be relatively Gaussian, reflecting a linear combinatio
of many predictors, adhering to the Central Limit Theorem. It is also
common to see a skewed distribution of fitted values, if one or more of
the predictors has a skewed distribution. This latter problem is usually
corrected before modelling using a variable transformation. Our
simulation assess this by using four different distributions to
represent fitted values: (1) uniform, (2) normal, (3) skewed and (4)
discrete. This is constructed by defining the raw predictor \(X_{raw}\)
in four corresponding distributions: (1) \(U(-1, 1)\), (2)
\(N(0, 0.3^2)\), (3) \(lognormal(0, 0.6^2)/3\) and (4) \(u\{1, 5\}\). We
would expect that the best reading of residual plots occurs when the
fitted values are uniformly distributed.

Three different sample sizes are used, \(n = 50, 100, 300\) across the
experiments. We would expect considerable variation in the signal
strength in the simulated data plots with smaller \(n\). A sample size
of 300 is typically enough for structure to be visible in a scatter plot
reliably.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/different-dist-1} 

}

\caption{Variations in fitted values, that might affect perception of residual plots. Four different distributions are used.}\label{fig:different-dist}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/different-n-1} 

}

\caption{Examining the effect of signal strength for the three different values of $n$ used in the experiment, for non-linear structure with fixed $\sigma = 1.5$, uniform fitted value distribution, and "S" shape. For these factor levels, only when $n = 300$ is the "S" shape clearly visible.}\label{fig:different-n}
\end{figure}

\hypertarget{experimental-setup}{%
\subsection{Experimental setup}\label{experimental-setup}}

\hypertarget{controlling-the-strength-of-the-signal}{%
\subsubsection{Controlling the strength of the
signal}\label{controlling-the-strength-of-the-signal}}

As summarised in Table \ref{tab:model-factor-table}, three additional
parameters \(n\), \(\sigma\) and \(b\) are used to control the strength
of the signal so that different difficulty levels of lineups are
generated, and therefore, the estimated power curve will be smooth and
continuous. Parameter \(\sigma \in \{0.5, 1, 2, 4\}\) and
\(b \in \{0.25, 1, 4, 16, 64\}\) are used in data collection periods I
and II respectively. Figure \ref{fig:different-sigma} and
\ref{fig:different-b} demonstrate the impact of these two parameters. A
large value of \(\sigma\) will increase the variation of the error of
the non-linearity model and decrease the visibility of the visual
pattern. The parameter \(b\) controls the standard deviation of the
error across the support of the predictor. Given \(x \neq a\), a larger
value of \(b\) will lead to a larger ratio of the variance at \(x\) to
the variance at \(x - a = 0\), making the visual pattern more obvious.

Three different sample sizes are used (n = 50, 100, 300) in all three
data collection periods. It can be observed from Figure
\ref{fig:different-n} that with fewer data points drawn in a residual
plot, the visual pattern is more difficult to be detected.

\hypertarget{effect-size}{%
\subsubsection{Effect size}\label{effect-size}}

Effect size in statistics measures the strength of the signal relative
to the noise. It is surprisingly difficult to quantify in general, even
for simulated data as used in this experiment.

For the non-linearity model, the key items defining effect size are
sample size (\(n\)) and variance of the error term (\(\sigma^2\)), and
so effect size would be roughly calculated as \(\sqrt{n}/{\sigma}\). As
sample size increases the effect size would increase, but as variance
increases the effect size decreases. However, it is not clear how the
additional parameter for the model polynomial order, \(k\), should be
incorporated. Intuitively, the large \(k\) means more complex pattern,
which likely means effect size would decrease. For the purposes of our
calculations we have chosen to use an approach based on Kullback-Leibler
divergence \citep{kullback1951information}, coupled with simulation.
This formulation defines effect size to be:

\[E = \frac{1}{2}\left(\boldsymbol{\mu}_z'(diag(\boldsymbol{R}\sigma^2))^{-1}\boldsymbol{\mu}_z\right)\]
\noindent where \(diag(.)\) is the diagonal matrix constructed from the
diagonal elements of a matrix,
\(\boldsymbol{R} = \boldsymbol{I}_n - \boldsymbol{H}\) is the residual
operator,
\(\boldsymbol{H} = \boldsymbol{X}(\boldsymbol{X}'\boldsymbol{X})^{-1}\boldsymbol{X}'\)
is the hat matrix,
\(\boldsymbol{\mu}_z = \boldsymbol{R}\boldsymbol{Z}\boldsymbol{\beta}_z\)
is the expected values of residuals with \(\boldsymbol{Z}\) be any
higher order terms of \(\boldsymbol{X}\) leave out by the regression
equation and \(\boldsymbol{\beta}_z\) be the corresponding coefficients,
and \(\sigma^2\boldsymbol{I}\) is the assumed covariance matrix of the
error term when \(H_0\) is true.

In the heteroskedasticity model, the key elements for measuring effect
size are sample size, \(n\), and the ratio of the biggest variance to
smallest variance, \(b\). Larger values of both would produce higher
effect size. However, it is not clear how to incorporate the additional
shape parameter, \(a\). Thus the same approach is used here, where the
formula can be written as:

\[E = \frac{1}{2}\left(log\frac{|diag(\boldsymbol{R}\boldsymbol{V}\boldsymbol{R}')|}{|diag(\boldsymbol{R})|} - n + tr(diag(\boldsymbol{R}\boldsymbol{V}\boldsymbol{R}')^{-1}diag(\boldsymbol{R}))\right)\]
\noindent where \(\boldsymbol{V}\) is the actual covariance matrix of
the error term. (Derivations for these equations are provided in the
Appendix.)

To compute the effect size for each lineup we simulate a sufficient
large number of samples from the same model, in each sample, the number
of observations \(n\) is fixed. We then compute the effect size for each
sample and take the average as the final value. This ensures lineups
constructed with the same experimental factors will share the same
effect size.

\hypertarget{subject-allocation}{%
\subsubsection{Subject allocation}\label{subject-allocation}}

As shown in Table \ref{tab:model-factor-table}, there are a total of
\(4 \times 4 \times 3 \times 4 = 192\) and
\(3 \times 5 \times 3 \times 4 = 180\) number of combinations of
parameter values for non-linearity model and heteroskedasticity model
respectively. Three replications are made for each of the combination
results in \(192 \times 3 = 576\) and \(180 \times 3 = 540\) lineups. In
addition, each lineup is designed to be evaluated by five different
subjects. After attempting some pilot studies internally, we decide to
present a block of 20 lineups to every subject. And to ensure the
quality of the survey data, two lineups with obvious visual patterns are
included as attention checks. Thus, \(576 \times 5 / (20-2) = 160\) and
\(540 \times 5 / (20-2) = 150\) subjects are recruited to satisfy the
design of the data collection period I and II respectively.

As mentioned in Section \ref{power-of-the-tests}, \(\alpha\) used in
Equation \ref{eq:pvalue-beta-binomial} needs to be estimated using null
lineups. Three replications are made for \(3 \times 4 = 12\)
combinations of common factors \(n\) and fitted value distribution,
results in \(12 \times 3 = 36\) lineups included in data collection
period III. In these lineups, the data of the data plot is generated
from a model with zero effect size, while the data of the 19 null plots
are generated using the same simulation method discussed in Section
\ref{visual-test-procedure-based-on-lineups}. This generation procedure
differs from the canonical Rorschach lineup procedure, which requires
that all 20 plots are generated directly from the null model. However,
these lineups serve the same fundamental purpose: to assess the number
of visually interesting plots generated under \(H_0\).

To account for the fact that our simulation method for these lineups is
not the Rorschach procedure, we use the method suggested in
\citet{vanderplas2021statistical} for typical lineups containing a data
plot to estimate \(\alpha\). (We have included a sensitivity analysis in
the Appendix to examine the impact of the variance of the \(\alpha\)
estimate on our findings.)

All lineups consist of only null plots are planned to be evaluated by 20
subjects. However, presenting only these lineups to subjects are
considered to be bad practices as subjects will lose interest quickly.
Therefore, we plan to collect 6 more evaluations on the 279 lineups with
uniform fitted value distribution, result in
\((36 \times 20 + 279 \times 3 \times 6) / (20-2) = 133\) subjects
recruited for data collection period III.

\hypertarget{collecting-results}{%
\subsubsection{Collecting results}\label{collecting-results}}

Subjects for all three data collection periods are recruited from an
crowdsourcing platform called Prolific \citep{palan2018prolific}.
Prescreening procedure is applied during the recruitment, subjects are
required to be fluent in English, with \(98\%\) minimum approval rate
and 10 minimum submissions in other studies.

During the experiment, every subject is presented with a block of 20
lineups. A lineup consists of a randomly placed data plot and 19 null
plots, which are all residual plots drawn with raw residuals on the
y-axis and fitted values on the x-axis. An additional horizontal red
line is added at \(y = 0\) as a helping line.

The data of the data plot is simulated from one of two models described
in Section \ref{simulating-departures-from-good-residuals}, while the
data of the remaining 19 null plots are generated by the residual
rotation technique discussed in Section
\ref{visual-test-procedure-based-on-lineups}.

In every lineup evaluation, the subject is asked to select one or more
plots that are most different from others, provide a reason for their
selections, and evaluate how different they think the selected plots are
from others. If there is no noticeable difference between plots in a
lineup, subjects are permitted to select zero plots without providing
the reason. No subject are shown the same lineup twice. Information
about preferred pronoun, age group, education, and previous experience
in visual experiments are also collected. A subject's submission is only
accepted if the data plot is identified for at least one attention
check. Data of rejected submissions are discarded automatically to
maintain the overall data quality.

\hypertarget{results}{%
\section{Results}\label{results}}

Data collection used a total of 1176 lineups, and resulted in a total of
7974 evaluations from 443 participants. Roughly half corresponded to the
two models, non-linearity and heteroskedasticiy, and the three
collection periods had similar numbers in each. Each participant
received two of the 24 attention check lineups which were used to filter
results of participants who were clearly not making an honest effort
(only 4 of 443). To estimate \(\alpha\) for calculating statistical
significance (see Section \ref{sig}) there were 720 evaluations of 36
null lineups. Neither the attention checks nor null lineups were used
for the subsequent analysis. The de-identified data,
\texttt{vi\_survey}, is made available in the R package,
\texttt{visage}.

The data was collected on lineups constructed from four different fitted
value distributions: uniform, normal, skewed and discrete. More data was
collected on the uniform (each evaluated by 11 participants) than the
others (each evaluated by 5 participants). The analysis in Sections
\ref{power-analysis}-\ref{hetero-analysis}, uses only results from
lineups generated with uniform fitted values, a total 3069 lineup
evaluations. This was decided in order to compare the conventional and
visual test performance for an optimal scenario. Section
\ref{effect-of-fitted-value-distributions} examines how the results may
be affected if the fitted value distribution was different.

\hypertarget{power-comparison-of-the-tests}{%
\subsection{\texorpdfstring{Power comparison of the
tests\textasciitilde{}\label{power-analysis}}{Power comparison of the tests\textasciitilde{}}}\label{power-comparison-of-the-tests}}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/polypower-1} 

}

\caption{Comparison of power between different tests for non-linear patterns (uniform fitted values only). The power curves are estimated using logistic regression, and the horizontal lines of dots represent non-detect and detect results from human observers for each lineup. The visual test has multiple power curves estimated from bootstrap samples. The row of scatterplots plots at the bottom are examples of residual plots corresponding to the specific effect sizes marked by vertical lines in the main plot.}\label{fig:polypower}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/heterpower-1} 

}

\caption{Comparison of power between different tests for heteroskedasticity patterns (uniform fitted values only). Main plot shows the power curves, with dots indicating non-detect and detect in human evaluations of lineups. The multiple lines for the visual test arise from estimating the power on many bootstrap samples. The row of scatterplots plots at the bottom are examples of residual plots corresponding to the specific effect sizes marked by vertical lines in the main plot.}\label{fig:heterpower}
\end{figure}

Figures \ref{fig:polypower} and \ref{fig:heterpower} present the power
curves of various tests plotted against the effect size in the
residuals, for non-linearity and heteroskedasticity, respectively. In
each case the power of visual test is re-calculated for multiple
bootstrap samples leading to the many (orange) curves. The effect size
was computed at a 5\% significance level and plotted on a natural
logarithmic scale. To facilitate visual calibration of effect size
values with the corresponding diagnostic plots, a sequence of example
residual plots with increasing effect sizes is provided at the bottom of
these figures. These plots serve as a visual aid to help readers
understand how different effect size values translate to changes in the
diagnostic plots. The horizontal lines of dots at 0 and 1 represent the
detects or non-detects made by human observers for each lineup.

Figures \ref{fig:polypower} compares the power for the different tests
for non-linear structure in the residuals. The test with the uniformly
higher power is the RESET test, one that specifically tests for
non-linearity. The power curves for the visual test are effectively a
shift right from that of the RESET test. This means that the RESET test
will reject a lower effect size (less structure) than the visual test,
but otherwise the performance will be similar. In other words, the RESET
test is more sensitive than the visual test. This is not necessarily a
good feature, for the purposes of diagnosing model defects. If one scans
the residual plot examples at the bottom, we might argue that the
non-linearity is not sufficiently problematic until an effect size of
around 3 or 3.5. The RESET test would reject closer to an effect size of
2, but the visual test would reject closer 3.25. As expected the
Breusch-Pagan and Shapiro-Wilk tests have much lower power - they are
not designed to detect non-linearity.

For the heteroskedasticity pattern, the power of Breusch-Pagan test,
designed for detecting heteroskedasticity, is uniformly higher than the
other tests. The visual test power curve is a right shift. This shows a
similar story to the power curves for non-linearity pattern - the
conventional test is more sensitive than the visual test. From the
example residual plots at the bottom we might argue that the
heteroskedasticity becomes noticeably visible around an effect size of 3
or 3.5. However the Breusch-Pagan test would reject at around effect
size 2.5. Interestingly, the power curve for the Shapiro-Wilk test (for
non-normality) is only sighly different to that of the visual test,
suggesting that it performs reasonably for detecting heteroskedasticity,
too. The power curve for the Breusch-Pagan test suggests it is not
useful for detecting heteroskeadsticity, as expected.

Overall, the results show that the conventional tests are more sensitive
than the visual test. The conventional tests do have higher power for
the patterns they are designed to detect, and generaly unable to detect
other patterns. The visual test doesn't require specifiying the pattern
ahead of time, relying purely on whether the observed residual plot is
detectably different from ``good'' residual plots. They will perform
equally well regardless of the type of model defect. This aligns with
the advice of experts on residual analysis, who consider residual plot
analysis to be an indispensable tool for diagnosing model problems. What
we gain from using a visual test for this purpose is the removal of
subjective argument about whether a pattern is visible or not. The
lineup protocol provides the calibration of detecting patterns, that if
the pattern in the data plot cannot be distinguished from patterns in
good residual plots, then no discernible problem with the model exists.

\hypertarget{comparison-of-test-decisions-based-on-p-values}{%
\subsection{\texorpdfstring{Comparison of test decisions based on
\(p\)-values\textasciitilde{}\label{p-value}}{Comparison of test decisions based on p-values\textasciitilde{}}}\label{comparison-of-test-decisions-based-on-p-values}}

The power comparison demonstrated that the appropriate conventional
tests will reject more aggressively than visual tests, but we don't know
how the decisions for each lineup would agree or disagree. Here we
compare the reject or fail to reject decisions of these tests, across
all the lineups. Figure \ref{fig:p-value-comparison} shows the agreement
of the conventional and visual tests using a mosaic plot for both
non-linearity patterns and heteroskedasticity patterns.

For both patterns the lineups resulting in a rejection by the visual
test are \emph{all} also rejected by the conventional test, except for
one from the heteroskedasticity model. This reflects exactly the story
from the previous section, that the conventional tests reject more
aggressively than the visual test.

For lineups containing non-linearity patterns, conventional tests reject
69\% and visual tests reject 32\% of the time. Of the lineups rejected
by the conventional test, 46\% are rejected by the visual test, that is,
approximately half as many as the conventional test. There are no
lineups that are rejected by the visual test but not by the conventional
test.

In terms of lineups containing heteroskedasticity patterns, 76\% are
rejected by conventional tests, while 56\% are rejected by visual tests.
When the conventional test rejects a lineup, there is a good chance
(73\%) the visual test will also reject it.

Surprisingly, the visual test rejects 1 of the 33 (3\%) of lineups where
the conventional test does not reject. Figure \ref{fig:heter-example}
shows this lineup. The data plot in position seventeen displays a
relatively strong heteroskedasticity pattern, and has a strong effect
size (\(log_e(E)=4.02\)). This is reflected by the visual test
\(p\text{-value} = 0.026\). But the Breusch-Pagan test
\(p\text{-value} = 0.056\), is slightly above the significance cutoff of
\(0.05\). This lineup was evaluated by 11 subjects, it has experimental
factors \(a = 0\) (``butterfly'' shape), \(b = 64\) (large variance
ratio), \(n = 50\) (small sample size), and a uniform distribution for
the fitted values. It may have been the small sample size and the
presence of a few outliers that may have resulted in the lack of
detection by the conventional test.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/p-value-comparison-1} 

}

\caption{Rejection rate ($p$-value $\leq0.05$) of visual test conditional on the conventional test decision on non-linearity (left) and heteroskedasticity (right) lineups (uniform fitted values only) displayed using a mosaic plot. The visual test rejects less frequently than the conventional test. We would generally expect the visual test to only reject when the conventional test does. Surprisingly, one lineup containing a heteroskedasticity pattern does not follow this rule. }\label{fig:p-value-comparison}
\end{figure}

\begin{figure}[t!]

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/heter-example-1} 

}

\caption{The single heteroskedasticity lineup that is rejected by the visual test but not by the BP test. The data plot at panel 17 contains a "butterfly" shape. It has effect size $ = 3.76$, somewhat surprising that it is not detected by the BP test.}\label{fig:heter-example}
\end{figure}

\hypertarget{effect-of-amount-of-non-linearity}{%
\subsection{\texorpdfstring{Effect of amount of
non-linearity\textasciitilde{}\label{nonlin-analysis}}{Effect of amount of non-linearity\textasciitilde{}}}\label{effect-of-amount-of-non-linearity}}

The order of the polynomial is a primary factor contributing to the
pattern produced by the non-linearity model. Figure
\ref{fig:poly-power-uniform-j} explores the relationship between
polynomial order and power of the tests. The conventional tests have
higher power for lower orders of Hermite polynomials, and it drops
substantially for the ``triple-U'' shape. To understand why this is, one
needs to return to the way the RESET test is applied. It requires a
parameter indicating degree of fitted values to test for, and the
recommendation is to generically use four \citep{ramsey_tests_1969}.
However, the ``triple-U'' shape is constructed from the Hermite
polynomials using power up to 18. If the RESET test had been applied
using a higher power no less than six, the power curve of ``triple-U''
shape will be closer to other power curves. This illustrates the
sensitivity of the conventional test to the parameter choice, and
highlights a limitation that it helps to know the data generating
process to set the parameters for the test, which is unrealistic.
However, we examined this in more detail (see Appendix) and found that
there is no harm for setting the parameter higher than four on the
tests' operation for lower order polynomial shapes. Using a parameter
value of six, instead of four, yields higher power regardless of
generating process, and would be recommended.

For visual tests, we expect the ``U'' shape to be detected more readily,
followed by the ``S'', ``M'' and ``triple-U'' shape. From Figure
\ref{fig:poly-power-uniform-j}, it can be observed that the power curves
mostly align with these expectations, except for the ``M'' shape, which
is as easy to be detected as the ``S'' shape. This suggests a benefit of
the visual test, the knowing the shape ahead of time is not needed for
its application.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/poly-power-uniform-j-1} 

}

\caption{The effect of the order of the polynomial on the power of conventional and visual tests. Deeper colour indicates higher order. The default RESET tests under-performs significantly in detecting the "triple-U" shape. To achieve a similar power as other shapes, a higher order polynomial parameter needs to be used for the RESET test, but this higher than the recommended value.}\label{fig:poly-power-uniform-j}
\end{figure}

\hypertarget{effect-of-shape-of-heteroskedasticity}{%
\subsection{\texorpdfstring{Effect of shape of
heteroskedasticity\textasciitilde{}\label{hetero-analysis}}{Effect of shape of heteroskedasticity\textasciitilde{}}}\label{effect-of-shape-of-heteroskedasticity}}

Figure \ref{fig:heter-power-uniform-a} examines the impact of the shape
of the heteroskedasticity on the power of of both tests. The butterfly
shape has higher power on both types of tests. The ``left-triangle'' and
the ``right-triangle'' shapes are functionally identical, and this is
observed for the conventional test, where the power curves are
identical. Interestingly there is a difference for the visual test,
where the power curve of the ``left-triangle'' shape is slightly higher
than that of the ``right-triangle'' shape. This indicates a bias in
perceiving heteroskedasticity depending on the direction. This would be
worth investigating further.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/heter-power-uniform-a-1} 

}

\caption{The effect of heteroskedasticity shape (parameter $a$) on the power of conventional and visual tests. The butterfly has higher power in both tests. Curiously, the visual test has slightly higher power for the "left-triangle" than the "right-triangle" shape, when it would be expected that they should be identical, which is observed in conventional testing.}\label{fig:heter-power-uniform-a}
\end{figure}

\hypertarget{effect-of-fitted-value-distributions}{%
\subsection{Effect of fitted value
distributions}\label{effect-of-fitted-value-distributions}}

In regression analysis, predictions are conditional on the observed
values of the predictors, that is, the conditional mean of the dependent
variable \(Y\) given the value of the independent variable \(X\),
\(E(Y|X)\). This is an often forgotten element of regression analysis
but it is important. Where \(X\) is observed, particularly the
distribution of the \(X\) values in the sample, or consequently
\(\hat{Y}\), may affect the ability to read any patterns in the residual
plots. This experiment was constructed to assess this, based on four
different distributions of fitted values: uniform, normal, discrete and
lognormal (skewed). We would expect that if \(\hat{Y}\) has a uniform
distribution, this would make it easier to read the relationship with
the residuals.

\begin{figure}[t!]

{\centering \includegraphics[width=1\linewidth]{paper_files/figure-latex/different-x-dist-poly-power-1} 

}

\caption{Comparison of power on lineups with different fitted value distributions for conventional and visual tests for non-linearity and heteroskedasticity patterns. The power curves of conventional tests for non-linearity and heteroskedasticity patterns are produced by RESET tests and Breusch-Pagan tests, respectively. Power curves of visual tests are estimated using five evaluations on each lineup. For lineups with a uniform fitted value distribution, the five evaluations are repeatedly sampled from the total eleven evaluations to give multiple power curves (grey). Surprisingly, the fitted value distribution has produces more variability in the power of conventional tests than visual tests. Uneven distributions, normal and lognormal distributions, tend to yield lower power.}\label{fig:different-x-dist-poly-power}
\end{figure}

Figure \ref{fig:different-x-dist-poly-power} examines the impact of the
fitted value distribution on the power of conventional (top row) and
visual (bottom row) tests for both the non-linearity (left column) and
heteroskedasticity (right column) patterns. For conventional tests, only
the power curves of appropriate tests which are RESET tests and
Breusch-Pagan tests for non-linearity pattern and heteroskedasticity
pattern respectively are shown. For visual tests, note that, more
evaluations on lineup with uniform fitted value distribution were
collected, so to have a fair comparison, we randomly sample five from
the total eleven evaluations to estimate the power curves. Re-sampling
produces the multiple curves for the uniform, and provides an indication
of the variation of the power.

Perhaps surprisingly, the visual tests have more consistent power across
the different fitted value distributions. For the non-linear pattern,
there is almost no power difference. For the heteroskedastic pattern,
uniform and discrete have higher power than normal and lognormal. The
likely reason is that these latter two have less observations in the
tails where the heteroskedastic pattern needs to be detected.

The variation in power in the conventional tests is at first sight,
shocking. However, it is discussed, albeit rarely, in the testing
literature. See, for example, \citet{jamshidian2007study},
\citet{olvera2019relationship} and \citet{zhang2018practical} which show
derivations and use simulation to assess the effect of the observed
distribution of the predictors on test power. The big differences in
power curves that are seen in Figure
\ref{fig:different-x-dist-poly-power} is echoed in the results reported
in these articles.

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

Motivated by the advice of regression analysis experts, that residual
plots as opposed to conventional tests are an indispensible methods for
assessing model fit, a human subjects experiment was conducted using
visual inference. The experiment tested two primary departures from good
residuals: non-linearity and heteroskedasticity.

The experiment found that conventional residual-based statistical tests
are more sensitive to weak residual departures from model assumptions
than visual tests as would be evaluated by humans. That is, conventional
tests conclude there are problems with the model fit almost twice as
often as humans would. They often reject when departures in the form of
non-linearity and heteroskedasticity are not visible to a human.

One might say that this is correct behaviour, but it can be argued that
the conventional tests are rejecting when it is not necessary. Many of
these rejections happen even when downstream analysis and results would
not be significantly affected by the small departures from a good fit.
The results from human evaluations provide a more practical solution,
which reinforces the statements from regression experts that residual
plots are an indispensible method for model diagnostics.

Now it is important to note that residual plots need to be delivered as
a lineup, where it is embedded in a field of null plots. A residual plot
may contains many visual features, but some are caused by the
characteristics of the predictors and the randomness of the error, not
by the violation of the model assumptions. These irrelevant visual
features have a chance to be filtered out by subjects with a comparison
to null plots, results in a set of more accurate visual findings. This
enables a careful calibration for reading structure in residual plots.

However, human evaluation of residuals is expensive. It is
time-consuming, laborious and unfriendly to vision-impaired people. This
is another reason why it often appears to be ignored. With the
availability of sophisticated computer vision algorithms today, the goal
of this work is to form the basis of providing automated residual plot
reading. The findings suggest the strong demand of graphical inspection
in regression diagnostics, so developing an automatic visual inference
system to evaluate lineups of residual plots is valuable. We plan to
build a completed open-source system in an \texttt{R} package and
provide a web interface such as a website for public to interact with.
Details about this system will be discussed in our next paper.

The experiment also revealed some interesting details about how residual
plots are read. For the most part, the visual test performed very
similarly to the appropriate conventional test only with the power curve
shifted in the less sensitive direction. Unlike the conventional tests,
where one needs to specifically test for non-linearity or
heteroskedasticity the visual test operated effectively across the range
of departures from good residuals.

As expected, if the fitted value distribution is not uniform, there is a
loss of power in the visual test. Structure is hardest to detect if
fitted values are lognormal. Also, complex structure are generally
harder to detect, but there are outliers. Under the designed scenarios
in this paper, we find the visual test to be a more robust test against
the change of fitted value distributions. A surprising finding was that
the direction of heteroskedasticity appears to affect the ability to
visually detect it, with wedge to the right being less detectable.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

These \texttt{R} packages are used for the work: \texttt{cli}
\citep{cli}, \texttt{curl} \citep{curl}, \texttt{dplyr} \citep{dplyr},
\texttt{ggplot2} \citep{ggplot2}, \texttt{jsonlite} \citep{jsonlite},
\texttt{lmtest} \citep{lmtest}, \texttt{mpoly} \citep{mpoly},
\texttt{progress} \citep{progress}, \texttt{tibble} \citep{tibble},
\texttt{ggmosaic} \citep{ggmosaic}, \texttt{purrr} \citep{purrr},
\texttt{tidyr} \citep{tidyr}, \texttt{readr} \citep{readr},
\texttt{stringr} \citep{stringr}, \texttt{here} \citep{here},
\texttt{kableExtra} \citep{kableextra}, \texttt{patchwork}
\citep{patchwork}, \texttt{rcartocolor} \citep{rcartocolor}. The study
website is powered by \texttt{PythonAnywhere} \citep{pythonanywhere} and
the \texttt{Flask} web framework \citep{flask}. The \texttt{jsPsych}
framework \citep{jspsych} is used to create behavioral experiments that
run in our study website.

The article was created with R packages \texttt{rticles}
\citep{rticles}, \texttt{knitr} \citep{knitr} and \texttt{rmarkdown}
\citep{rmarkdown}. The project's Github repository (link here) contains
all materials required to reproduce this article.

\hypertarget{supplementary-material}{%
\section*{Supplementary material}\label{supplementary-material}}
\addcontentsline{toc}{section}{Supplementary material}

The supplementary material is available at (link here). It includes more
details about the experimental setup, the derivation of the effect size,
the effect of data collection period, and the estimate of \(\alpha\).

\bibliographystyle{tfcad}
\bibliography{paper.bib}





\end{document}
